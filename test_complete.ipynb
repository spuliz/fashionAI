{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "args = easydict.EasyDict({\n",
    "        \"learning_rate\":1e-3,\n",
    "        \"learning_rate_D\":1e-4,\n",
    "        \"learning_rate_D_local\":1e-4,\n",
    "        \"gan\":\"lsgan\",\n",
    "        \"model\":\"scribbler\",\n",
    "        \"num_epoch\":100,\n",
    "        \"feature_weight\":0,\n",
    "        \"global_pixel_weight_l\":0,\n",
    "        \"local_pixel_weight_l\":1,\n",
    "        \"pixel_weight_ab\":0,\n",
    "        \"pixel_weight_rgb\":0,\n",
    "        \"discriminator_weight\":0,\n",
    "        \"discriminator_local_weight\":0,\n",
    "        \"style_weight\":0,\n",
    "        \"visualize_every\":10,\n",
    "        \"batchsize\": 100,\n",
    "        \"epoch\": 20,\n",
    "        \"gpu\": [0],\n",
    "        \"gpu\": 1,\n",
    "        \"display_port\":8097,\n",
    "        \"data_path\":\"/training_handbags_pretrain/\",\n",
    "        \"save_dir\":\"/test\",\n",
    "        \"load_dir\":\"/test\",\n",
    "        \"save_every\":1000,\n",
    "        \"load_epoch\":-1,\n",
    "        \"load_epoch\":-1,\n",
    "        \"load_D\":-1,\n",
    "        \"image_size\":128,\n",
    "        \"resize_to\":300,\n",
    "        \"resize_max\":1,\n",
    "        \"resize_min\":0.6,\n",
    "        \"patch_size_min\":20,\n",
    "        \"patch_size_max\":40,\n",
    "        \"batch_size\":32,\n",
    "        \"num_input_texture_patch\":2,\n",
    "        \"num_local_texture_patch\":1,\n",
    "        \"color_space\":\"lab\",\n",
    "        \"threshold_D_max\":0.8,\n",
    "        \"content_layers\":\"relu4_2\",\n",
    "        \"style_layers\": \"relu3_2, relu4_2\",\n",
    "        \"use_segmentation_patch\": True,\n",
    "        \"input_texture_patch\": \"dtd_texture\",\n",
    "        \"loss_texture\": \"dtd_texture\",\n",
    "        \"local_texture_size\": 50,\n",
    "        \"texture_discrminator_loss\": True,\n",
    "        \"tv_weight\":1,\n",
    "        \"mode\":\"texture\",\n",
    "        \"visualize_mode\": \"train\",\n",
    "        \"crop\":\"random\",\n",
    "        \"contrast\": True,\n",
    "        \"occlude\": False,\n",
    "        \"checkpoints_path\": \"data/\",\n",
    "        \"noise_gen\": False,\n",
    "        \"absolute_load\": \"\",\n",
    "        \"out\": \"result\",\n",
    "        \"resume\": False,\n",
    "        \"unit\": 1000\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy command\n",
    "command = '--display_port 7770 --load 0 --load_D -1 --load_epoch 105 --gpu 2 --model texturegan --feature_weight 1e2 --pixel_weight_ab 1e3 --global_pixel_weight_l 1e3 --local_pixel_weight_l 0 --style_weight 0 --discriminator_weight 1e3 --discriminator_local_weight 1e6  --learning_rate 1e-4 --learning_rate_D 1e-4 --batch_size 36 --save_every 50 --num_epoch 100000 --save_dir /home/psangkloy3/skip_leather_re/ --load_dir /home/psangkloy3/skip_leather_re/ --data_path ../../training_handbags_pretrain/ --learning_rate_D_local  1e-4 --local_texture_size 50 --patch_size_min 20 --patch_size_max 40 --num_input_texture_patch 1 --visualize_every 5 --num_local_texture_patch 1'\n",
    "args = parse_arguments(command.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import os.path as osp\n",
    "import random\n",
    "\n",
    "\n",
    "IMG_EXTENSIONS = [\n",
    "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n",
    "]\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "\n",
    "def find_classes(directory):\n",
    "    classes = [d for d in os.listdir(directory) if osp.isdir(os.path.join(directory, d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "\n",
    "def make_dataset(directory, opt, erode_seg=True):\n",
    "    # opt: 'train' or 'val'\n",
    "    img = glob.glob(osp.join(directory, opt + '_img/*/*.jpg'))\n",
    "    img = sorted(img)\n",
    "    skg = glob.glob(osp.join(directory, opt + '_skg/*/*.jpg'))\n",
    "    skg = sorted(skg)\n",
    "    seg = glob.glob(osp.join(directory, opt + '_seg/*/*.jpg'))\n",
    "    seg = sorted(seg)\n",
    "    txt = glob.glob(osp.join(directory, opt + '_txt/*/*.jpg'))\n",
    "    #txt = glob.glob(osp.join(directory, opt + '_dtd_txt/*/*.jpg'))\n",
    "    extended_txt = []\n",
    "    #import pdb; pdb.set_trace()\n",
    "    for i in range(len(skg)):\n",
    "        extended_txt.append(txt[i%len(txt)])\n",
    "    random.shuffle(extended_txt)\n",
    "    \n",
    "\n",
    "    if erode_seg:\n",
    "        eroded_seg = glob.glob(osp.join(directory, 'eroded_' + opt + '_seg/*/*.jpg'))\n",
    "        eroded_seg = sorted(eroded_seg)\n",
    "        return list(zip(img, skg, seg , eroded_seg, extended_txt))\n",
    "    else:\n",
    "        return list(zip(img, skg, seg, extended_txt))\n",
    "\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')\n",
    "\n",
    "\n",
    "def accimage_loader(path):\n",
    "    import accimage\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "\n",
    "def default_loader(path):\n",
    "    return pil_loader(path)\n",
    "\n",
    "\n",
    "class ImageFolder(data.Dataset):\n",
    "    def __init__(self, opt, root, transform=None, target_transform=None,\n",
    "                 loader=default_loader, erode_seg=True):\n",
    "     \n",
    "        self.root = root\n",
    "        self.imgs = make_dataset(root, opt, erode_seg=erode_seg)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "        self.erode_seg = erode_seg\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.erode_seg:\n",
    "            img_path, skg_path, seg_path, eroded_seg_path, txt_path = self.imgs[index]\n",
    "        else:\n",
    "            img_path, skg_path, seg_path, txt_path = self.imgs[index]\n",
    "        \n",
    "        img = self.loader(img_path)\n",
    "        skg = self.loader(skg_path)\n",
    "        seg = self.loader(seg_path)\n",
    "        txt = self.loader(txt_path)\n",
    "\n",
    "        if self.erode_seg:\n",
    "            eroded_seg = self.loader(eroded_seg_path)\n",
    "        else:\n",
    "            eroded_seg = None\n",
    "\n",
    "        if self.transform is not None:\n",
    "            if self.erode_seg:\n",
    "                img, skg, seg, eroded_seg, txt = self.transform([img, skg, seg, eroded_seg, txt])\n",
    "            else:\n",
    "                img, skg, seg, txt = self.transform([img, skg, seg, txt])\n",
    "                eroded_seg = seg\n",
    "\n",
    "        return img, skg, seg, eroded_seg, txt\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import torchvision.transforms\n",
    "import torch\n",
    "import math\n",
    "import random\n",
    "from PIL import Image, ImageOps\n",
    "from skimage import color\n",
    "try:\n",
    "    import accimage\n",
    "except ImportError:\n",
    "    accimage = None\n",
    "import numpy as np\n",
    "import numbers\n",
    "import types\n",
    "import collections\n",
    "\n",
    "class toLAB(object):\n",
    "    \"\"\"\n",
    "    Transform to convert loaded into LAB space. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.space = 'LAB'\n",
    "        \n",
    "    def __call__(self, images):\n",
    "        lab_images = [color.rgb2lab(np.array(image)/255.0) for image in images]\n",
    "        return lab_images\n",
    "\n",
    "\n",
    "class toRGB_(object):\n",
    "    \"\"\"\n",
    "    Transform to convert loaded into LAB space. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.space = 'LAB'\n",
    "        \n",
    "    def __call__(self, images):\n",
    "        images = np.transpose(images.numpy(), (1, 2, 0))\n",
    "        rgb_images = [(np.array(image)/255.0) for image in images]\n",
    "        return rgb_images\n",
    "\n",
    "\n",
    "class toRGB(object):\n",
    "    \"\"\"\n",
    "    Transform to convert loaded into RGB color space. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, space ='LAB'):\n",
    "        self.space = space\n",
    "        \n",
    "    def __call__(self, images):\n",
    "        if self.space =='LAB':\n",
    "            # npimg = np.transpose(np.array(images), (1, 2, 0))\n",
    "            # print(image)\n",
    "            rgb_img = [np.transpose(color.lab2rgb(np.transpose(image, (1,2,0))), (2,0,1)) for image in images]\n",
    "        elif self.space =='RGB':\n",
    "            # print np.shape(images)\n",
    "            # images = np.transpose(images.numpy(), (1, 2, 0))\n",
    "            rgb_img = [(np.array(image)/255.0) for image in images]\n",
    "\n",
    "        return rgb_img\n",
    "\n",
    "\n",
    "class toTensor(object):\n",
    "    \"\"\"Transforms a Numpy image to torch tensor\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.space = 'RGB'\n",
    "        \n",
    "    def __call__(self, pics):\n",
    "        imgs = [torch.from_numpy(pic.transpose((2, 0, 1))) for pic in pics]\n",
    "        return imgs\n",
    "\n",
    "\n",
    "def normalize_lab(lab_img):\n",
    "    \"\"\"\n",
    "    Normalizes the LAB image to lie in range 0-1\n",
    "    \n",
    "    Args:\n",
    "    lab_img : torch.Tensor img in lab space\n",
    "    \n",
    "    Returns:\n",
    "    lab_img : torch.Tensor Normalized lab_img \n",
    "    \"\"\"\n",
    "    mean = torch.zeros(lab_img.size())\n",
    "    stds = torch.zeros(lab_img.size())\n",
    "    \n",
    "    mean[:,0,:,:] = 50\n",
    "    mean[:,1,:,:] = 0\n",
    "    mean[:,2,:,:] = 0\n",
    "    \n",
    "    stds[:,0,:,:] = 50\n",
    "    stds[:,1,:,:] = 128\n",
    "    stds[:,2,:,:] = 128\n",
    "    \n",
    "    return (lab_img.double() - mean.double())/stds.double()\n",
    "\n",
    "def normalize_seg(seg):\n",
    "    \"\"\"\n",
    "    Normalizes the LAB image to lie in range 0-1\n",
    "    \n",
    "    Args:\n",
    "    lab_img : torch.Tensor img in lab space\n",
    "    \n",
    "    Returns:\n",
    "    lab_img : torch.Tensor Normalized lab_img \n",
    "    \"\"\"\n",
    "    result = seg[:,0,:,:]\n",
    "    if torch.max(result) >1:\n",
    "        result = result/100.0\n",
    "    result = torch.round(result)\n",
    "    \n",
    "    \n",
    "    return result\n",
    "\n",
    "def normalize_rgb(rgb_img):\n",
    "    \"\"\"\n",
    "    Normalizes the LAB image to lie in range 0-1\n",
    "    \n",
    "    Args:\n",
    "    lab_img : torch.Tensor img in lab space\n",
    "    \n",
    "    Returns:\n",
    "    lab_img : torch.Tensor Normalized lab_img \n",
    "    \"\"\"\n",
    "    mean = torch.zeros(rgb_img.size())\n",
    "    stds = torch.zeros(rgb_img.size())\n",
    "    \n",
    "    mean[:,0,:,:] = 0.485\n",
    "    mean[:,1,:,:] = 0.456\n",
    "    mean[:,2,:,:] = 0.406\n",
    "    \n",
    "    stds[:,0,:,:] = 0.229\n",
    "    stds[:,1,:,:] = 0.224\n",
    "    stds[:,2,:,:] = 0.225\n",
    "    \n",
    "    return (rgb_img.double() - mean.double())/stds.double()\n",
    "   \n",
    "    \n",
    "def denormalize_lab(lab_img):\n",
    "    \"\"\"\n",
    "    Normalizes the LAB image to lie in range 0-1\n",
    "    \n",
    "    Args:\n",
    "    lab_img : torch.Tensor img in lab space\n",
    "    \n",
    "    Returns:\n",
    "    lab_img : torch.Tensor Normalized lab_img \n",
    "    \"\"\"\n",
    "    mean = torch.zeros(lab_img.size())\n",
    "    stds = torch.zeros(lab_img.size())\n",
    "    \n",
    "    mean[:,0,:,:] = 50\n",
    "    mean[:,1,:,:] = 0\n",
    "    mean[:,2,:,:] = 0\n",
    "    \n",
    "    stds[:,0,:,:] = 50\n",
    "    stds[:,1,:,:] = 128\n",
    "    stds[:,2,:,:] = 128\n",
    "\n",
    "    return lab_img.double() *stds.double() + mean.double()\n",
    "\n",
    "\n",
    "def denormalize_rgb(rgb_img):\n",
    "    \"\"\"\n",
    "    Normalizes the LAB image to lie in range 0-1\n",
    "    \n",
    "    Args:\n",
    "    lab_img : torch.Tensor img in lab space\n",
    "    \n",
    "    Returns:\n",
    "    lab_img : torch.Tensor Normalized lab_img \n",
    "    \"\"\"\n",
    "    mean = torch.zeros(rgb_img.size())\n",
    "    stds = torch.zeros(rgb_img.size())\n",
    "    \n",
    "    mean[:,0,:,:] = 0.485\n",
    "    mean[:,1,:,:] = 0.456\n",
    "    mean[:,2,:,:] = 0.406\n",
    "    \n",
    "    stds[:,0,:,:] = 0.229\n",
    "    stds[:,1,:,:] = 0.224\n",
    "    stds[:,2,:,:] = 0.225\n",
    "\n",
    "    return rgb_img.double() *stds.double() + mean.double()\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# multiple images transformation -- based on transform from torchvision\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    \"\"\"Composes several transforms together.\n",
    "    Args:\n",
    "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>> ])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, imgs):\n",
    "        for t in self.transforms:\n",
    "            imgs = t(imgs)\n",
    "        return imgs\n",
    "\n",
    "class Scale(object):\n",
    "    \"\"\"Rescale multiple input PIL.Image to the given size.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size. If size is a sequence like\n",
    "            (w, h), output size will be matched to this. If size is an int,\n",
    "            smaller edge of the image will be matched to this number.\n",
    "            i.e, if height > width, then image will be rescaled to\n",
    "            (size * height / width, size)\n",
    "        interpolation (int, optional): Desired interpolation. Default is\n",
    "            ``PIL.Image.BILINEAR``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        assert isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "        self.transform = torchvision.transforms.Scale(size)\n",
    "\n",
    "    def __call__(self, imgs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            imgs (list of PIL.Image): Images to be scaled.\n",
    "        Returns:\n",
    "            list of PIL.Image: Rescaled images.\n",
    "        \"\"\"       \n",
    "        return [self.transform(img) for img in imgs]\n",
    "\n",
    "\n",
    "class CenterCrop(object):\n",
    "    \"\"\"Crops the given PIL.Image at the center.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an\n",
    "            int instead of sequence like (h, w), a square crop (size, size) is\n",
    "            made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "        self.transform = torchvision.transforms.CenterCrop(size)\n",
    "\n",
    "    def __call__(self, imgs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            imgs (PIL.Image): Image to be cropped.\n",
    "        Returns:\n",
    "            PIL.Image: Cropped image.\n",
    "        \"\"\"\n",
    "        return [self.transform(img) for img in imgs]\n",
    "\n",
    "\n",
    "class Pad(object):\n",
    "    \"\"\"Pad the given PIL.Image on all sides with the given \"pad\" value.\n",
    "    Args:\n",
    "        padding (int or tuple): Padding on each border. If a single int is provided this\n",
    "            is used to pad all borders. If tuple of length 2 is provided this is the padding\n",
    "            on left/right and top/bottom respectively. If a tuple of length 4 is provided\n",
    "            this is the padding for the left, top, right and bottom borders\n",
    "            respectively.\n",
    "        fill: Pixel fill value. Default is 0. If a tuple of\n",
    "            length 3, it is used to fill R, G, B channels respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, padding, fill=0):\n",
    "        assert isinstance(padding, (numbers.Number, tuple))\n",
    "        assert isinstance(fill, (numbers.Number, str, tuple))\n",
    "        if isinstance(padding, collections.Sequence) and len(padding) not in [2, 4]:\n",
    "            raise ValueError(\"Padding must be an int or a 2, or 4 element tuple, not a \" +\n",
    "                             \"{} element tuple\".format(len(padding)))\n",
    "\n",
    "        self.padding = padding\n",
    "        self.fill = fill\n",
    "        \n",
    "        self.transform = torchvision.transforms.Pad(padding,fill)\n",
    "\n",
    "    def __call__(self, imgs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be padded.\n",
    "        Returns:\n",
    "            PIL.Image: Padded image.\n",
    "        \"\"\"\n",
    "        \n",
    "        return [self.transform(img) for img in imgs]\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop the given PIL.Image at a random location.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an\n",
    "            int instead of sequence like (h, w), a square crop (size, size) is\n",
    "            made.\n",
    "        padding (int or sequence, optional): Optional padding on each border\n",
    "            of the image. Default is 0, i.e no padding. If a sequence of length\n",
    "            4 is provided, it is used to pad left, top, right, bottom borders\n",
    "            respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, padding=0):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, imgs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be cropped.\n",
    "        Returns:\n",
    "            PIL.Image: Cropped image.\n",
    "        \"\"\"\n",
    "        if self.padding > 0:\n",
    "            imgs = [ImageOps.expand(img, border=self.padding, fill=0) for img in imgs]\n",
    "\n",
    "        w, h = imgs[0].size\n",
    "        th, tw = self.size\n",
    "        if w == tw and h == th:\n",
    "            return imgs\n",
    "\n",
    "        x1 = random.randint(0, w - tw)\n",
    "        y1 = random.randint(0, h - th)\n",
    "        return [img.crop((x1, y1, x1 + tw, y1 + th)) for img in imgs]\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    \"\"\"Horizontally flip the given PIL.Image randomly with a probability of 0.5.\"\"\"\n",
    "\n",
    "    def __call__(self, imgs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be flipped.\n",
    "        Returns:\n",
    "            PIL.Image: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        if random.random() < 0.5:\n",
    "            return [img.transpose(Image.FLIP_LEFT_RIGHT) for img in imgs]\n",
    "        return imgs\n",
    "\n",
    "\n",
    "class RandomSizedCrop(object):\n",
    "    \"\"\"Crop the given PIL.Image to random size and aspect ratio.\n",
    "    A crop of random size of (0.08 to 1.0) of the original size and a random\n",
    "    aspect ratio of 3/4 to 4/3 of the original aspect ratio is made. This crop\n",
    "    is finally resized to given size.\n",
    "    This is popularly used to train the Inception networks.\n",
    "    Args:\n",
    "        size: size of the smaller edge\n",
    "        interpolation: Default: PIL.Image.BILINEAR\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, min_resize=0.08,max_resize=1.0,interpolation=Image.BILINEAR):\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "        self.resize_size = (min_resize,max_resize)\n",
    "\n",
    "    def __call__(self, imgs):\n",
    "        for attempt in range(10):\n",
    "            area = imgs[0].size[0] * imgs[0].size[1]\n",
    "            target_area = random.uniform(self.resize_size[0], self.resize_size[1]) * area\n",
    "            aspect_ratio = random.uniform(3. / 4, 4. / 3)\n",
    "\n",
    "            w = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "            h = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                w, h = h, w\n",
    "\n",
    "            if w <= imgs[0].size[0] and h <= imgs[0].size[1]:\n",
    "                x1 = random.randint(0, imgs[0].size[0] - w)\n",
    "                y1 = random.randint(0, imgs[0].size[1] - h)\n",
    "\n",
    "                imgs = [img.crop((x1, y1, x1 + w, y1 + h)) for img in imgs]\n",
    "                assert([img.size == (w, h) for img in imgs])\n",
    "\n",
    "                return [img.resize((self.size, self.size), self.interpolation) for img in imgs]\n",
    "\n",
    "        # Fallback\n",
    "        scale = Scale(self.size, interpolation=self.interpolation)\n",
    "        crop = CenterCrop(self.size)\n",
    "        return crop(scale(imgs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Scribbler(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, ngf):\n",
    "        \"\"\"\n",
    "        Defines the necessary modules of the Scribbler Generator\n",
    "        Input:\n",
    "        - int input_nc : Input number of channels\n",
    "        - int output_nc : Output number of channels\n",
    "        \"\"\"\n",
    "        super(Scribbler, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d\n",
    "        self.batch_norm = nn.BatchNorm2d\n",
    "        self.ngf = ngf\n",
    "\n",
    "        self.res_block = ResidualBlock\n",
    "        self.biup = UpsamplingBlock\n",
    "        self.model = self.create_model(input_nc,output_nc)\n",
    "\n",
    "\n",
    "    def create_model(self, input_nc, output_nc):\n",
    "        \"\"\"\n",
    "        Function which pieces together the model\n",
    "        \"\"\"\n",
    "        model = nn.Sequential()\n",
    "        ngf=self.ngf\n",
    "\n",
    "        model.add_module('conv_1', self.conv(input_nc,ngf,3,1,1))\n",
    "        model.add_module('batch_1', self.batch_norm(ngf))\n",
    "        model.add_module('norm_1', nn.ReLU(True))\n",
    "\n",
    "\n",
    "        model.add_module('res_block_1', self.res_block(ngf,ngf))\n",
    "        model.add_module('conv_2',self.conv(ngf,ngf*2,3,2,1))\n",
    "        model.add_module('batch_2',self.batch_norm(ngf*2))\n",
    "        model.add_module('norm_2',nn.ReLU(True))\n",
    "\n",
    "        model.add_module('res_block_2',self.res_block(ngf*2,ngf*2))\n",
    "\n",
    "        model.add_module('conv_3', self.conv(ngf*2, ngf*4, 3, 2, 1))\n",
    "        model.add_module('batch_3', self.batch_norm(ngf*4))\n",
    "        model.add_module('norm_3', nn.ReLU(True))\n",
    "\n",
    "        model.add_module('res_block_3',self.res_block(ngf*4,ngf*4))\n",
    "\n",
    "        model.add_module('conv_4', self.conv(ngf*4,ngf*8,3,2,1))\n",
    "        model.add_module('batch_4', self.batch_norm(ngf*8))\n",
    "        model.add_module('norm_4', nn.ReLU(True))\n",
    "\n",
    "        model.add_module('res_block_4',self.res_block(ngf*8,ngf*8))\n",
    "        model.add_module('res_block_5',self.res_block(ngf*8,ngf*8))\n",
    "        model.add_module('res_block_6',self.res_block(ngf*8,ngf*8))\n",
    "        model.add_module('res_block_7',self.res_block(ngf*8,ngf*8))\n",
    "        model.add_module('res_block_8',self.res_block(ngf*8,ngf*8))\n",
    "\n",
    "        model.add_module('upsampl_1',self.biup(ngf*8,ngf*4,3,1,1))\n",
    "        model.add_module('batch_5',self.batch_norm(ngf*4))\n",
    "        model.add_module('norm_5',nn.ReLU(True))\n",
    "        model.add_module('res_block_9',self.res_block(ngf*4,ngf*4))\n",
    "        model.add_module('res_block_10',self.res_block(ngf*4,ngf*4))\n",
    "\n",
    "        model.add_module('upsampl_2',self.biup(ngf*4,ngf*2,3,1,1))\n",
    "        model.add_module('batch_6',self.batch_norm(ngf*2))\n",
    "        model.add_module('norm_6',nn.ReLU(True))\n",
    "        model.add_module('res_block_11',self.res_block(ngf*2,ngf*2))\n",
    "        model.add_module('res_block_12',self.res_block(ngf*2,ngf*2))\n",
    "\n",
    "        model.add_module('upsampl_3',self.biup(ngf*2,ngf,3,1,1))\n",
    "        model.add_module('batch_7',self.batch_norm(ngf))\n",
    "        model.add_module('norm_7',nn.ReLU(True))\n",
    "        model.add_module('res_block_13',self.res_block(ngf,ngf))\n",
    "        model.add_module('batch_8',self.batch_norm(ngf))\n",
    "\n",
    "        model.add_module('res_block_14',self.res_block(ngf,ngf))\n",
    "        model.add_module('conv_5',self.conv(ngf,3,3,1,1))\n",
    "\n",
    "        model.add_module('batch_9',self.batch_norm(3))\n",
    "\n",
    "        return model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class UpsamplingBlock(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, kernel, stride, pad):\n",
    "        \"\"\"\n",
    "        Single block of upsampling operation\n",
    "        Input:\n",
    "        - int input_nc    : Input number of channels\n",
    "        - int output_nc   : Output number of channels\n",
    "        - int kernel      : Kernel size\n",
    "        - int stride\t  : Stride length\n",
    "        - int pad         : Padd_moduleing\n",
    "        \"\"\"\n",
    "        super(UpsamplingBlock, self).__init__()\n",
    "\n",
    "        conv = nn.Conv2d\n",
    "        biup = nn.UpsamplingBilinear2d\n",
    "\n",
    "        block = nn.Sequential()\n",
    "        block.add_module('conv_1', conv(input_nc, output_nc, kernel, stride,pad))\n",
    "        block.add_module('upsample_2', biup(scale_factor=2))\n",
    "\n",
    "        self.biup_block = block\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.biup_block(x)\n",
    "\n",
    "# 3x3 Convolution\n",
    "def conv3x3(in_channels, out_channels, stride=1, padding=1, dilation=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                     stride=stride, padding=padding, dilation=dilation)\n",
    "\n",
    "\n",
    "    # Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None,\n",
    "                 dilation=(1, 1), residual=True):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride,\n",
    "                             padding=dilation[0], dilation=dilation[0])\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels, stride,\n",
    "                             padding=dilation[1], dilation=dilation[1])\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        if self.residual:\n",
    "            out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.legacy as legacy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc, ndf, use_sigmoid):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.input_nc = input_nc\n",
    "        self.ndf = ndf\n",
    "        self.conv = nn.Conv2d\n",
    "        self.batch_norm = nn.BatchNorm2d\n",
    "        self.res_block = ResidualBlock\n",
    "\n",
    "        self.model = self.create_discriminator(use_sigmoid)\n",
    "\n",
    "    def create_discriminator(self, use_sigmoid):\n",
    "        norm_layer = self.batch_norm\n",
    "        ndf = self.ndf  # 32\n",
    "        self.res_block = ResidualBlock\n",
    "        \n",
    "        sequence = [\n",
    "            nn.Conv2d(self.input_nc, self.ndf, kernel_size=9, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            nn.Conv2d(self.ndf, self.ndf * 2, kernel_size=5, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            nn.Conv2d(self.ndf * 2, self.ndf * 8, kernel_size=5, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            self.res_block(self.ndf * 8, self.ndf * 8),\n",
    "            self.res_block(self.ndf * 8, self.ndf * 8),\n",
    "\n",
    "            nn.Conv2d(self.ndf * 8, self.ndf * 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv2d(self.ndf * 4, 1, kernel_size=4, stride=2, padding=1)\n",
    "        ]\n",
    "\n",
    "        if use_sigmoid:\n",
    "            sequence += [nn.Sigmoid()]\n",
    "\n",
    "        return nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class LocalDiscriminator(nn.Module):\n",
    "    def __init__(self, input_nc, ndf, use_sigmoid):\n",
    "        super(LocalDiscriminator, self).__init__()\n",
    "\n",
    "        self.input_nc = input_nc\n",
    "        self.ndf = ndf\n",
    "        self.conv = nn.Conv2d\n",
    "        self.batch_norm = nn.BatchNorm2d\n",
    "        self.res_block = ResidualBlock\n",
    "\n",
    "        self.model = self.create_discriminator(use_sigmoid)\n",
    "\n",
    "    def create_discriminator(self, use_sigmoid):\n",
    "        norm_layer = self.batch_norm\n",
    "        ndf = self.ndf  # 32\n",
    "        self.res_block = ResidualBlock\n",
    "        \n",
    "        sequence = [\n",
    "            nn.Conv2d(self.input_nc, self.ndf, kernel_size=3, stride=2, padding=1),nn.InstanceNorm2d(ndf),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            nn.Conv2d(self.ndf, self.ndf * 4, kernel_size=3, stride=2, padding=1),nn.InstanceNorm2d(ndf* 4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            #nn.Conv2d(self.ndf * 2, self.ndf * 8, kernel_size=5, stride=2, padding=1),\n",
    "            #nn.LeakyReLU(0.2, True),\n",
    "            #nn.Dropout(0.2),\n",
    "            \n",
    "            self.res_block(self.ndf * 4, self.ndf * 4),\n",
    "            self.res_block(self.ndf * 4, self.ndf * 4),\n",
    "\n",
    "            nn.Conv2d(self.ndf * 4, self.ndf * 2, kernel_size=3, stride=2, padding=1), nn.InstanceNorm2d(ndf* 2),\n",
    "            #nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv2d(self.ndf * 2, 1, kernel_size=3, stride=2, padding=1)\n",
    "        ]\n",
    "\n",
    "        if use_sigmoid:\n",
    "            sequence += [nn.Sigmoid()]\n",
    "\n",
    "        return nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class NLayerDiscriminator(nn.Module):\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, gpu_ids=[]):\n",
    "        super(NLayerDiscriminator, self).__init__()\n",
    "        self.gpu_ids = gpu_ids\n",
    "\n",
    "        kw = 4\n",
    "        padw = int(np.ceil((kw-1)/2))\n",
    "        sequence = [\n",
    "            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2**n, 8)\n",
    "            sequence += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2,\n",
    "                          padding=padw), norm_layer(ndf * nf_mult,\n",
    "                                                    affine=True), nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2**n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1,\n",
    "                      padding=padw), norm_layer(ndf * nf_mult,\n",
    "                                                affine=True), nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n",
    "\n",
    "        if use_sigmoid:\n",
    "            sequence += [nn.Sigmoid()]\n",
    "\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if len(self.gpu_ids)  and isinstance(input.data, torch.cuda.FloatTensor):\n",
    "            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n",
    "        else:\n",
    "            return self.model(input)\n",
    "\n",
    "# 3x3 Convolution\n",
    "def conv3x3(in_channels, out_channels, stride=1, padding=1, dilation=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                     stride=stride, padding=padding, dilation=dilation)\n",
    "\n",
    "\n",
    "# Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None,\n",
    "                 dilation=(1, 1), residual=True):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride,\n",
    "                             padding=dilation[0], dilation=dilation[0])\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels, stride, \n",
    "                             padding=dilation[1], dilation=dilation[1])\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        if self.residual:\n",
    "            out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TextureGAN(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, ngf):\n",
    "        \"\"\"\n",
    "        Defines the necessary modules of the TextureGAN Generator\n",
    "        Input:\n",
    "        - int input_nc : Input number of channels\n",
    "        - int output_nc : Output number of channels\n",
    "        \"\"\"\n",
    "        super(TextureGAN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d\n",
    "        self.batch_norm = nn.BatchNorm2d\n",
    "        self.ngf = ngf\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "\n",
    "        self.res_block = ResidualBlock\n",
    "        self.biup = UpsamplingBlock\n",
    "        self.main_model = MainModel\n",
    "        self.model = self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        skip_block = nn.Sequential()\n",
    "\n",
    "        skip_block.add_module('main_model', self.main_model(self.input_nc, self.output_nc, self.ngf))\n",
    "        skip_block.add_module('conv_6', self.conv(self.ngf+5, self.ngf*2, 3, 1, 1))\n",
    "        skip_block.add_module('res_block_14', self.res_block(self.ngf*2,self.ngf*2))\n",
    "        skip_block.add_module('res_block_15', self.res_block(self.ngf*2,self.ngf*2))\n",
    "        skip_block.add_module('conv_7', self.conv(self.ngf*2, 3, 3, 1, 1))\n",
    "        skip_block.add_module('batch_9', self.batch_norm(3))\n",
    "\n",
    "        return skip_block\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class MainModel(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, ngf):\n",
    "        \"\"\"\n",
    "        Function which pieces together the model\n",
    "        \"\"\"\n",
    "        super(MainModel, self).__init__()\n",
    "        self.conv = nn.Conv2d\n",
    "        self.batch_norm = nn.BatchNorm2d\n",
    "        self.ngf = ngf\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "\n",
    "        self.res_block = ResidualBlock\n",
    "        self.biup = UpsamplingBlock\n",
    "        model = nn.Sequential()\n",
    "        \n",
    "        model.add_module('conv_1', self.conv(input_nc,ngf,3,1,1))\n",
    "        model.add_module('batch_1', self.batch_norm(ngf))\n",
    "        model.add_module('norm_1', nn.ReLU(True))\n",
    "\n",
    "        model.add_module('res_block_1', self.res_block(ngf,ngf))\n",
    "        model.add_module('conv_2', self.conv(ngf,ngf*2,3,2,1))\n",
    "        model.add_module('batch_2',self.batch_norm(ngf*2))\n",
    "        model.add_module('norm_2', nn.ReLU(True))\n",
    "\n",
    "        model.add_module('res_block_2', self.res_block(ngf*2,ngf*2))\n",
    "\n",
    "        model.add_module('conv_3',self.conv(ngf*2,ngf*4,3,2,1))\n",
    "        model.add_module('batch_3',self.batch_norm(ngf*4))\n",
    "        model.add_module('norm_3',nn.ReLU(True))\n",
    "\n",
    "        model.add_module('res_block_3',self.res_block(ngf*4,ngf*4))\n",
    "\n",
    "        model.add_module('conv_4',self.conv(ngf*4,ngf*8,3,2,1))\n",
    "        model.add_module('batch_4',self.batch_norm(ngf*8))\n",
    "        model.add_module('norm_4',nn.ReLU(True))\n",
    "        \n",
    "        model.add_module('res_block_4',self.res_block(ngf*8,ngf*8))\n",
    "        model.add_module('res_block_5',self.res_block(ngf*8,ngf*8))\n",
    "        model.add_module('res_block_6',self.res_block(ngf*8,ngf*8))\n",
    "        model.add_module('res_block_7',self.res_block(ngf*8,ngf*8))\n",
    "        model.add_module('res_block_8',self.res_block(ngf*8,ngf*8))\n",
    "\n",
    "        model.add_module('upsampl_1',self.biup(ngf*8,ngf*4,3,1,1))\n",
    "        model.add_module('batch_5',self.batch_norm(ngf*4))\n",
    "        model.add_module('norm_5',nn.ReLU(True))\n",
    "        model.add_module('res_block_9',self.res_block(ngf*4,ngf*4))\n",
    "        model.add_module('res_block_10',self.res_block(ngf*4,ngf*4))\n",
    "\n",
    "        model.add_module('upsampl_2',self.biup(ngf*4,ngf*2,3,1,1))\n",
    "        model.add_module('batch_6',self.batch_norm(ngf*2))\n",
    "        model.add_module('norm_6',nn.ReLU(True))\n",
    "        model.add_module('res_block_11',self.res_block(ngf*2,ngf*2))\n",
    "        model.add_module('res_block_12',self.res_block(ngf*2,ngf*2))\n",
    "\n",
    "        model.add_module('upsampl_3',self.biup(ngf*2,ngf,3,1,1))\n",
    "        model.add_module('batch_7',self.batch_norm(ngf))\n",
    "        model.add_module('norm_7',nn.ReLU(True))\n",
    "        model.add_module('res_block_13',self.res_block(ngf,ngf))\n",
    "        model.add_module('batch_8',self.batch_norm(ngf))\n",
    "\n",
    "        self.main_model = model\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat((self.main_model(x), x), 1)\n",
    "        #return self.main_model(input)\n",
    "\n",
    "\n",
    "class UpsamplingBlock(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, kernel, stride, pad):\n",
    "        \"\"\"\n",
    "        Single block of upsampling operation\n",
    "        Input:\n",
    "        - int input_nc    : Input number of channels\n",
    "        - int output_nc   : Output number of channels\n",
    "        - int kernel      : Kernel size\n",
    "        - int stride\t  : Stride length\n",
    "        - int pad         : Padd_moduleing\n",
    "        \"\"\"\n",
    "        super(UpsamplingBlock, self).__init__()\n",
    "\n",
    "        conv = nn.Conv2d\n",
    "        biup = nn.Upsample\n",
    "\n",
    "        block = nn.Sequential()\n",
    "        block.add_module('conv_1', conv(input_nc, output_nc, kernel, stride, pad))\n",
    "        block.add_module('upsample_2', biup(scale_factor=2, mode='bilinear'))\n",
    "\n",
    "        self.biup_block = block\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.biup_block(x)\n",
    "\n",
    "\n",
    "# 3x3 Convolution\n",
    "def conv3x3(in_channels, out_channels, stride=1, padding=1, dilation=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                     stride=stride, padding=padding, dilation=dilation)\n",
    "\n",
    "# Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None,\n",
    "                 dilation=(1, 1), residual=True):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride,\n",
    "                             padding=dilation[0])\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels, stride, \n",
    "                             padding=dilation[1])\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        if self.residual:\n",
    "            out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ScribblerDilate128(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, ngf):\n",
    "        \"\"\"\n",
    "        Defines the necessary modules of the Scribbler Generator\n",
    "        Input:\n",
    "        - int input_nc : Input number of channels\n",
    "        - int output_nc : Output number of channels\n",
    "        \"\"\"\n",
    "        super(ScribblerDilate128, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d\n",
    "        self.batch_norm = nn.BatchNorm2d\n",
    "        self.ngf = ngf\n",
    "\n",
    "        self.res_block = ResidualBlock\n",
    "        self.dilate_block = DilationBlock\n",
    "        self.biup = UpsamplingBlock\n",
    "        self.concat = ConcatTable\n",
    "        self.model = self.create_model(input_nc,output_nc)\n",
    "\n",
    "    def create_test_model(self, input_nc, output_nc):\n",
    "        \"\"\"\n",
    "        Function which pieces together the model\n",
    "        \"\"\"\n",
    "\n",
    "        model = nn.Sequential()\n",
    "        ngf=self.ngf\n",
    "        #model.add_module('identity',nn.Identity())\n",
    "        model.add_module('res_block_1', self.res_block(output_nc))\n",
    "        #model.add_module('res_block_2', self.res_block(output_nc))\n",
    "\n",
    "        #model.add_module('tanh',nn.Tanh())\n",
    "        return model\n",
    "        #model.add_module('batch_9',self.batch_norm(3)) #?? why?\n",
    "\n",
    "    def create_model(self,input_nc,output_nc):\n",
    "        \"\"\"\n",
    "        Function which pieces together the model\n",
    "        \"\"\"\n",
    "\n",
    "        model = nn.Sequential()\n",
    "        ngf = self.ngf\n",
    "\n",
    "        model.add_module('conv_1',self.dilate_block(input_nc,ngf))\n",
    "        model.add_module('batch_1',self.batch_norm(ngf))\n",
    "        model.add_module('norm_1',nn.ReLU(True))\n",
    "\n",
    "        #skip connection here\n",
    "        block1 = nn.Sequential()\n",
    "\n",
    "        block1.add_module('res_block_1', self.res_block(ngf))\n",
    "\n",
    "        block1.add_module('conv_2',self.conv(ngf,ngf*2,3,2,1))\n",
    "        block1.add_module('batch_2',self.batch_norm(ngf*2))\n",
    "        block1.add_module('norm_2',nn.ReLU(True))\n",
    "\n",
    "        block1.add_module('res_block_2',self.res_block(ngf*2))\n",
    "\n",
    "        block1.add_module('conv_3',self.conv(ngf*2,ngf*4,3,2,1))\n",
    "        block1.add_module('batch_3',self.batch_norm(ngf*4))\n",
    "        block1.add_module('norm_3',nn.ReLU(True))\n",
    "\n",
    "        block1.add_module('res_block_3',self.res_block(ngf*4))\n",
    "\n",
    "        block1.add_module('conv_4',self.conv(ngf*4,ngf*8,3,1,1))\n",
    "        block1.add_module('batch_4',self.batch_norm(ngf*8))\n",
    "        block1.add_module('norm_4',nn.ReLU(True))\n",
    "\n",
    "        block1.add_module('res_block_4',self.res_block(ngf*8))\n",
    "        block1.add_module('res_block_5',self.res_block(ngf*8))\n",
    "        block1.add_module('res_block_6',self.res_block(ngf*8))\n",
    "        block1.add_module('res_block_7',self.res_block(ngf*8))\n",
    "        block1.add_module('res_block_8',self.res_block(ngf*8))\n",
    "\n",
    "        block1.add_module('upsampl_1',self.biup(ngf*8,ngf*4,3,1,1,dil=1))\n",
    "        block1.add_module('batch_5',self.batch_norm(ngf*4))\n",
    "        block1.add_module('norm_5',nn.ReLU(True))\n",
    "        block1.add_module('res_block_9',self.res_block(ngf*4))\n",
    "        #model.add_module('res_block_10',self.res_block(ngf*4))\n",
    "\n",
    "        block1.add_module('upsampl_2',self.biup(ngf*4,ngf*2,3,1,1,dil=1))\n",
    "        block1.add_module('batch_6',self.batch_norm(ngf*2))\n",
    "        block1.add_module('norm_6',nn.ReLU(True))\n",
    "        block1.add_module('res_block_11',self.res_block(ngf*2))\n",
    "        #model.add_module('res_block_12',self.res_block(ngf*2))\n",
    "        block1.add_module('conv_7',self.conv(ngf*2,ngf,3,1,1))\n",
    "        block1.add_module('batch_7',self.batch_norm(ngf))\n",
    "        block1.add_module('norm_7',nn.ReLU(True))\n",
    "\n",
    "        #block1.add_module('upsampl_3',self.biup(ngf*2,ngf,5,1,1,dil=1))\n",
    "        #block1.add_module('batch_7',self.batch_norm(ngf))\n",
    "        #block1.add_module('norm_7',nn.ReLU(True))\n",
    "\n",
    "        #skip connection here\n",
    "        block2 = nn.Sequential()\n",
    "        block2.add_module('res_block_13',self.res_block(ngf))\n",
    "        block2.add_module('res_block_14',self.res_block(ngf))\n",
    "        block2.add_module('res_block_15',self.res_block(ngf))\n",
    "        mlp = self.concat(block1,block2)\n",
    "        model.add_module('concat',mlp)\n",
    "        model.add_module('upsampl_4',self.biup(2*ngf,3,3,1,1,dil=3))\n",
    "        # model.add_module('batch_8',self.batch_norm(ngf))\n",
    "        # model.add_module('norm_8',nn.ReLU(True))\n",
    "        model.add_module('tanh',nn.Tanh())\n",
    "        # model.add_module('conv_5',self.conv(ngf,3,3,1,1))\n",
    "\n",
    "        return model\n",
    "        # model.add_module('batch_9',self.batch_norm(3)) #?? why?\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "\n",
    "class UpsamplingBlock(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, kernel, stride, pad, dil):\n",
    "        '''\n",
    "        Single block of upsampling operation\n",
    "        Input:\n",
    "        - int input_nc    : Input number of channels\n",
    "        - int output_nc   : Output number of channels\n",
    "        - int kernel      : Kernel size\n",
    "        - int stride\t  : Stride length\n",
    "        - int pad         : Padd_moduleing\n",
    "        '''\n",
    "        super(UpsamplingBlock, self).__init__()\n",
    "\n",
    "        conv = nn.Conv2d\n",
    "        biup = nn.UpsamplingBilinear2d\n",
    "\n",
    "        block = nn.Sequential()\n",
    "        block.add_module('conv_1',conv(input_nc, output_nc, kernel, stride, pad, dilation=dil))\n",
    "        block.add_module('upsample_2',biup(scale_factor=2))\n",
    "\n",
    "        self.biup_block = block\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.biup_block(input)\n",
    "\n",
    "\n",
    "class DilationBlock(nn.Module):\n",
    "    def __init__(self,input_c,output_c):\n",
    "        '''\n",
    "        Single block of upsampling operation\n",
    "        Input:\n",
    "        - int input_nc    : Input number of channels\n",
    "        - int output_nc   : Output number of channels\n",
    "        - int kernel      : Kernel size\n",
    "        - int stride\t  : Stride length\n",
    "        - int pad         : Padd_moduleing\n",
    "        '''\n",
    "        super(DilationBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d\n",
    "        self.batch_norm = nn.BatchNorm2d\n",
    "\n",
    "        self.dilblock = nn.Sequential()\n",
    "\n",
    "        self.dilblock.add_module('conv_1',self.conv(input_c,output_c,5,1,2,5))\n",
    "        self.dilblock.add_module('batch_1',self.batch_norm(output_c))\n",
    "        self.dilblock.add_module('norm_1',nn.ReLU(True))\n",
    "\n",
    "        self.dilblock.add_module('conv_2',self.conv(output_c,output_c,5,1,1,5))\n",
    "        self.dilblock.add_module('batch_2',self.batch_norm(output_c))\n",
    "        self.dilblock.add_module('norm_2',nn.ReLU(True))\n",
    "\n",
    "        self.dilblock.add_module('conv_3',self.conv(output_c,output_c,5,1,1,5))\n",
    "        self.dilblock.add_module('batch_3',self.batch_norm(output_c))\n",
    "        self.dilblock.add_module('norm_3',nn.ReLU(True))\n",
    "\n",
    "        self.dilblock.add_module('conv_4',self.conv(output_c,output_c,3,1,1,5))\n",
    "        self.dilblock.add_module('batch_4',self.batch_norm(output_c))\n",
    "\n",
    "\n",
    "    def forward(self,input):\n",
    "        return self.dilblock(input)#+input\n",
    "\n",
    "\n",
    "class ConcatTable(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(ConcatTable, self).__init__()\n",
    "        self.layer1 = model1\n",
    "        self.layer2 = model2\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = [self.layer1(x), self.layer2(x)]\n",
    "        z = torch.cat((y[0], y[1]),1)\n",
    "        return z\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, block_size):\n",
    "        '''\n",
    "        Residual block for bottleneck operation\n",
    "        Input:\n",
    "        - int block_size : number of features in the bottleneck layer\n",
    "        '''\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d\n",
    "        self.batch_norm = nn.BatchNorm2d\n",
    "\n",
    "        self.resblock = nn.Sequential()\n",
    "\n",
    "        self.resblock.add_module('conv_1',self.conv(block_size, block_size, 3, 1, 1, 1))\n",
    "        self.resblock.add_module('batch_1',self.batch_norm(block_size))\n",
    "        self.resblock.add_module('norm_1',nn.ReLU(True))\n",
    "\n",
    "        self.resblock.add_module('conv_2',self.conv(block_size, block_size, 3, 1, 1, 1))\n",
    "        self.resblock.add_module('batch_2',self.batch_norm(block_size))\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.resblock(input)+input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "        \n",
    "class localDiscriminator(nn.Module):\n",
    "    def __init__(self, input_nc, ndf, use_sigmoid):\n",
    "        super(localDiscriminator, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(input_nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# from . import transforms\n",
    "\n",
    "\n",
    "def vis_patch(img, skg, texture_location, color='lab'):\n",
    "    batch_size, _, _, _ = img.size()\n",
    "    if torch.cuda.is_available():\n",
    "        img = img.cpu()\n",
    "        skg = skg.cpu()\n",
    "\n",
    "    img = img.numpy()\n",
    "    skg = skg.numpy()\n",
    "\n",
    "    if color == 'lab':\n",
    "        ToRGB = toRGB()\n",
    "        \n",
    "    elif color =='rgb':\n",
    "        ToRGB = toRGB('RGB')\n",
    "        \n",
    "    img_np = ToRGB(img)\n",
    "    skg_np = ToRGB(skg)\n",
    "\n",
    "    vis_skg = np.copy(skg_np)\n",
    "    vis_img = np.copy(img_np)\n",
    "\n",
    "    # print np.shape(vis_skg)\n",
    "    for i in range(batch_size):\n",
    "        for text_loc in texture_location[i]:\n",
    "            xcenter, ycenter, size = text_loc\n",
    "            xcenter = max(xcenter-int(size/2),0) + int(size/2)\n",
    "            ycenter = max(ycenter-int(size/2),0) + int(size/2)\n",
    "            vis_skg[\n",
    "                i, :,\n",
    "                int(xcenter-size/2):int(xcenter+size/2),\n",
    "                int(ycenter-size/2):int(ycenter+size/2)\n",
    "            ] = vis_img[\n",
    "                    i, :,\n",
    "                    int(xcenter-size/2):int(xcenter+size/2),\n",
    "                    int(ycenter-size/2):int(ycenter+size/2)\n",
    "                ]\n",
    "\n",
    "    return vis_skg\n",
    "    \n",
    "def vis_image(img, color='lab'):\n",
    "    if torch.cuda.is_available():\n",
    "        img = img.cpu()\n",
    "\n",
    "    img = img.numpy()\n",
    "\n",
    "    if color == 'lab':\n",
    "        ToRGB = toRGB()\n",
    "    elif color =='rgb':\n",
    "        ToRGB = toRGB('RGB')\n",
    "\n",
    "    # print np.shape(img)\n",
    "    img_np = ToRGB(img)\n",
    "\n",
    "    return img_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/74/dbed754c0abd63768d3a7a7b472da35b08ac442cf87d73d5850a6f32391e/graphviz-0.13.2-py2.py3-none-any.whl\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.13.2\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def make_dot(var, params=None):\n",
    "    \"\"\" Produces Graphviz representation of PyTorch autograd graph\n",
    "    Blue nodes are the Variables that require grad, orange are Tensors\n",
    "    saved for backward in torch.autograd.Function\n",
    "    Args:\n",
    "        var: output Variable\n",
    "        params: dict of (name, Variable) to add names to node that\n",
    "            require grad (TODO: make optional)\n",
    "    \"\"\"\n",
    "    if params is not None:\n",
    "        assert isinstance(params.values()[0], Variable)\n",
    "        param_map = {id(v): k for k, v in params.items()}\n",
    "\n",
    "    node_attr = dict(style='filled',\n",
    "                     shape='box',\n",
    "                     align='left',\n",
    "                     fontsize='12',\n",
    "                     ranksep='0.1',\n",
    "                     height='0.2')\n",
    "    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "    seen = set()\n",
    "\n",
    "    def size_to_str(size):\n",
    "        return '(' + ', '.join(['%d' % v for v in size])+')'\n",
    "\n",
    "    def add_nodes(var):\n",
    "        if var not in seen:\n",
    "            if torch.is_tensor(var):\n",
    "                dot.node(str(id(var)), size_to_str(var.size()), fillcolor='orange')\n",
    "            elif hasattr(var, 'variable'):\n",
    "                u = var.variable\n",
    "                name = param_map[id(u)] if params is not None else ''\n",
    "                node_name = '%s\\n %s' % (name, size_to_str(u.size()))\n",
    "                dot.node(str(id(var)), node_name, fillcolor='lightblue')\n",
    "            else:\n",
    "                dot.node(str(id(var)), str(type(var).__name__))\n",
    "            seen.add(var)\n",
    "            if hasattr(var, 'next_functions'):\n",
    "                for u in var.next_functions:\n",
    "                    if u[0] is not None:\n",
    "                        dot.edge(str(id(u[0])), str(id(var)))\n",
    "                        add_nodes(u[0])\n",
    "            if hasattr(var, 'saved_tensors'):\n",
    "                for t in var.saved_tensors:\n",
    "                    dot.edge(str(id(t)), str(id(var)))\n",
    "                    add_nodes(t)\n",
    "    add_nodes(var.grad_fn)\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "# from utils import transforms as transforms\n",
    "# from models import save_network, GramMatrix\n",
    "# from utils.visualize import vis_image, vis_patch\n",
    "import time\n",
    "#import cv2\n",
    "import math\n",
    "import random\n",
    "\n",
    "def rand_between(a, b):\n",
    "    return a + torch.round(torch.rand(1) * (b - a))[0]\n",
    "\n",
    "\n",
    "def gen_input(img, skg, ini_texture, ini_mask, xcenter=64, ycenter=64, size=40):\n",
    "    # generate input skg with random patch from img\n",
    "    # input img,skg [bsx3xwxh], xcenter,ycenter, size\n",
    "    # output bsx5xwxh\n",
    "\n",
    "    w, h = img.size()[1:3]\n",
    "    # print w,h\n",
    "    xstart = max(int(xcenter - size / 2), 0)\n",
    "    ystart = max(int(ycenter - size / 2), 0)\n",
    "    xend = min(int(xcenter + size / 2), w)\n",
    "    yend = min(int(ycenter + size / 2), h)\n",
    "\n",
    "    input_texture = ini_texture  # torch.ones(img.size())*(1)\n",
    "    input_sketch = skg[0:1, :, :]  # L channel from skg\n",
    "    input_mask = ini_mask  # torch.ones(input_sketch.size())*(-1)\n",
    "\n",
    "    input_mask[:, xstart:xend, ystart:yend] = 1\n",
    "\n",
    "    input_texture[:, xstart:xend, ystart:yend] = img[:, xstart:xend, ystart:yend].clone()\n",
    "\n",
    "    return torch.cat((input_sketch.cpu().float(), input_texture.float(), input_mask), 0)\n",
    "\n",
    "def get_coor(index, size):\n",
    "    index = int(index)\n",
    "    #get original coordinate from flatten index for 3 dim size\n",
    "    w,h = size\n",
    "    \n",
    "    return ((index%(w*h))/h, ((index%(w*h))%h))\n",
    "\n",
    "def gen_input_rand(img, skg, seg, size_min=40, size_max=60, num_patch=1):\n",
    "    # generate input skg with random patch from img\n",
    "    # input img,skg [bsx3xwxh], xcenter,ycenter, size\n",
    "    # output bsx5xwxh\n",
    "    \n",
    "    bs, c, w, h = img.size()\n",
    "    results = torch.Tensor(bs, 5, w, h)\n",
    "    texture_info = []\n",
    "\n",
    "    # text_info.append([xcenter,ycenter,crop_size])\n",
    "    seg = seg / torch.max(seg) #make sure it's 0/1\n",
    "    \n",
    "    seg[:,0:int(math.ceil(size_min/2)),:] = 0\n",
    "    seg[:,:,0:int(math.ceil(size_min/2))] = 0\n",
    "    seg[:,:,int(math.floor(h-size_min/2)):h] = 0\n",
    "    seg[:,int(math.floor(w-size_min/2)):w,:] = 0\n",
    "    \n",
    "    counter = 0\n",
    "    for i in range(bs):\n",
    "        counter = 0\n",
    "        ini_texture = torch.ones(img[0].size()) * (1)\n",
    "        ini_mask = torch.ones((1, w, h)) * (-1)\n",
    "        temp_info = []\n",
    "        \n",
    "        for j in range(num_patch):\n",
    "            crop_size = int(rand_between(size_min, size_max))\n",
    "            \n",
    "            seg_index_size = seg[i,:,:].view(-1).size()[0]\n",
    "            seg_index = torch.arange(0,seg_index_size)\n",
    "            seg_one = seg_index[seg[i,:,:].view(-1)==1]\n",
    "            if len(seg_one) != 0:\n",
    "                seg_select_index = int(rand_between(0,seg_one.view(-1).size()[0]-1))\n",
    "                x,y = get_coor(seg_one[seg_select_index],seg[i,:,:].size())\n",
    "            else:\n",
    "                x,y = (w/2, h/2)\n",
    "            \n",
    "            temp_info.append([x, y, crop_size])\n",
    "            res = gen_input(img[i], skg[i], ini_texture, ini_mask, x, y, crop_size)\n",
    "\n",
    "            ini_texture = res[1:4, :, :]\n",
    "\n",
    "        texture_info.append(temp_info)\n",
    "        results[i, :, :, :] = res\n",
    "    return results, texture_info\n",
    "\n",
    "def gen_local_patch(patch_size, batch_size, eroded_seg, seg, img):\n",
    "    # generate local loss patch from eroded segmentation\n",
    "    \n",
    "    bs, c, w, h = img.size()\n",
    "    texture_patch = img[:, :, 0:patch_size, 0:patch_size].clone()\n",
    "\n",
    "    if patch_size != -1:\n",
    "        eroded_seg[:,0,0:int(math.ceil(patch_size/2)),:] = 0\n",
    "        eroded_seg[:,0,:,0:int(math.ceil(patch_size/2))] = 0\n",
    "        eroded_seg[:,0,:,int(math.floor(h-patch_size/2)):h] = 0\n",
    "        eroded_seg[:,0,int(math.floor(w-patch_size/2)):w,:] = 0\n",
    "\n",
    "    for i_bs in range(bs):\n",
    "                \n",
    "        i_bs = int(i_bs)\n",
    "        seg_index_size = eroded_seg[i_bs,0,:,:].view(-1).size()[0]\n",
    "        seg_index = torch.arange(0,seg_index_size).cuda()\n",
    "        #import pdb; pdb.set_trace()\n",
    "        #print bs, batch_size\n",
    "        seg_one = seg_index[eroded_seg[i_bs,0,:,:].view(-1)==1]\n",
    "        if len(seg_one) != 0:\n",
    "            random_select = int(rand_between(0, len(seg_one)-1))\n",
    "            #import pdb; pdb.set_trace()\n",
    "            \n",
    "            x,y = get_coor(seg_one[random_select], eroded_seg[i_bs,0,:,:].size())\n",
    "            #print x,y,i_bs\n",
    "        else:\n",
    "            x,y = (w/2, h/2)\n",
    "\n",
    "        if patch_size == -1:\n",
    "            xstart = 0\n",
    "            ystart = 0\n",
    "            xend = -1\n",
    "            yend = -1\n",
    "\n",
    "        else:\n",
    "            xstart = int(x-patch_size/2)\n",
    "            ystart = int(y-patch_size/2)\n",
    "            xend = int(x+patch_size/2)\n",
    "            yend = int(y+patch_size/2)\n",
    "\n",
    "        k = 1\n",
    "        while torch.sum(seg[i_bs,0,xstart:xend,ystart:yend]) < k*patch_size*patch_size:\n",
    "                \n",
    "            try:\n",
    "                k = k*0.9\n",
    "                if len(seg_one) != 0:\n",
    "                    random_select = int(rand_between(0, len(seg_one)-1))\n",
    "            \n",
    "                    x,y = get_coor(seg_one[random_select], eroded_seg[i_bs,0,:,:].size())\n",
    "            \n",
    "                else:\n",
    "                    x,y = (w/2, h/2)\n",
    "                xstart = (int)(x-patch_size/2)\n",
    "                ystart = (int)(y-patch_size/2)\n",
    "                xend = (int)(x+patch_size/2)\n",
    "                yend = (int)(y+patch_size/2)\n",
    "            except:\n",
    "                break\n",
    "                \n",
    "            \n",
    "        texture_patch[i_bs,:,:,:] = img[i_bs, :, xstart:xend, ystart:yend]\n",
    "        \n",
    "    return texture_patch\n",
    "\n",
    "def renormalize(img):\n",
    "    \"\"\"\n",
    "    Renormalizes the input image to meet requirements for VGG-19 pretrained network\n",
    "    \"\"\"\n",
    "\n",
    "    forward_norm = torch.ones(img.data.size()) * 0.5\n",
    "    forward_norm = Variable(forward_norm.cuda())\n",
    "    img = (img * forward_norm) + forward_norm  # add previous norm\n",
    "    # return img\n",
    "    mean = img.data.new(img.data.size())\n",
    "    std = img.data.new(img.data.size())\n",
    "    mean[:, 0, :, :] = 0.485\n",
    "    mean[:, 1, :, :] = 0.456\n",
    "    mean[:, 2, :, :] = 0.406\n",
    "    std[:, 0, :, :] = 0.229\n",
    "    std[:, 1, :, :] = 0.224\n",
    "    std[:, 2, :, :] = 0.225\n",
    "    img -= Variable(mean)\n",
    "    img = img / Variable(std)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize_training(netG, val_loader,input_stack, target_img, target_texture,segment, vis, loss_graph, args):\n",
    "    imgs = []\n",
    "    for ii, data in enumerate(val_loader, 0):\n",
    "        img, skg, seg, eroded_seg, txt = data  # LAB with negeative value\n",
    "        if random.random() < 0.5:\n",
    "            txt = img\n",
    "        # this is in LAB value 0/100, -128/128 etc\n",
    "        img = normalize_lab(img)\n",
    "        skg = normalize_lab(skg)\n",
    "        txt = normalize_lab(txt)\n",
    "        seg = normalize_seg(seg)\n",
    "        eroded_seg = normalize_seg(eroded_seg)\n",
    "        \n",
    "        bs, w, h = seg.size()\n",
    "        \n",
    "        seg = seg.view(bs, 1, w, h)\n",
    "        seg = torch.cat((seg, seg, seg), 1)\n",
    "        \n",
    "        eroded_seg = eroded_seg.view(bs, 1, w, h)\n",
    "        eroded_seg = torch.cat((eroded_seg, eroded_seg, eroded_seg), 1)\n",
    "\n",
    "        temp = torch.ones(seg.size()) * (1 - seg).float()\n",
    "        temp[:, 1, :, :] = 0  # torch.ones(seg[:,1,:,:].size())*(1-seg[:,1,:,:]).float()\n",
    "        temp[:, 2, :, :] = 0  # torch.ones(seg[:,2,:,:].size())*(1-seg[:,2,:,:]).float()\n",
    "\n",
    "        txt = txt.float() * seg.float() + temp\n",
    "      \n",
    "        patchsize = args.local_texture_size\n",
    "        batch_size = bs\n",
    "              \n",
    "        # seg=transforms.normalize_lab(seg)\n",
    "        # norm to 0-1 minus mean\n",
    "        if not args.use_segmentation_patch:\n",
    "            seg.fill_(1)\n",
    "            #skg.fill_(0)\n",
    "            eroded_seg.fill_(1)\n",
    "        if args.input_texture_patch == 'original_image':\n",
    "            inp, texture_loc = gen_input_rand(img, skg, eroded_seg[:, 0, :, :] * 100,\n",
    "                                              args.patch_size_min, args.patch_size_max,\n",
    "                                              args.num_input_texture_patch)\n",
    "        elif args.input_texture_patch == 'dtd_texture':\n",
    "            inp, texture_loc = gen_input_rand(txt, skg, eroded_seg[:, 0, :, :] * 100,\n",
    "                                              args.patch_size_min, args.patch_size_max,\n",
    "                                              args.num_input_texture_patch)\n",
    "\n",
    "        img = img.cuda()\n",
    "        skg = skg.cuda()\n",
    "        seg = seg.cuda()\n",
    "        eroded_seg = eroded_seg.cuda()\n",
    "        txt = txt.cuda()\n",
    "        inp = inp.cuda()\n",
    "\n",
    "        inp.size()\n",
    "\n",
    "        input_stack.resize_as_(inp.float()).copy_(inp)\n",
    "        target_img.resize_as_(img.float()).copy_(img)\n",
    "        segment.resize_as_(seg.float()).copy_(seg)\n",
    "        target_texture.resize_as_(txt.float()).copy_(txt)\n",
    "\n",
    "        inputv = Variable(input_stack)\n",
    "        targetv = Variable(target_img)\n",
    "        \n",
    "        gtimgv = Variable(target_img)\n",
    "        segv = Variable(segment)\n",
    "        txtv = Variable(target_texture)\n",
    "\n",
    "        outputG = netG(inputv)\n",
    "        \n",
    "        outputl, outputa, outputb = torch.chunk(outputG, 3, dim=1)\n",
    "        #outputlll = (torch.cat((outputl, outputl, outputl), 1))\n",
    "        gtl, gta, gtb = torch.chunk(gtimgv, 3, dim=1)\n",
    "        txtl, txta, txtb = torch.chunk(txtv, 3, dim=1)\n",
    "        \n",
    "        gtab = torch.cat((gta, gtb), 1)\n",
    "        txtab= torch.cat((txta, txtb), 1)\n",
    "        \n",
    "        if args.color_space == 'lab':\n",
    "            outputlll = (torch.cat((outputl, outputl, outputl), 1))\n",
    "            gtlll = (torch.cat((gtl, gtl, gtl), 1))\n",
    "            txtlll = torch.cat((txtl, txtl, txtl), 1)\n",
    "        elif args.color_space == 'rgb':\n",
    "            outputlll = outputG  # (torch.cat((outputl,outputl,outputl),1))\n",
    "            gtlll = gtimgv  # (torch.cat((targetl,targetl,targetl),1))\n",
    "            txtlll = txtv\n",
    "        if args.loss_texture == 'original_image':\n",
    "            targetl = gtl\n",
    "            targetab = gtab\n",
    "            targetlll = gtlll\n",
    "        else:\n",
    "            targetl = txtl\n",
    "            targetab = txtab\n",
    "            targetlll = txtlll\n",
    "       # import pdb; pdb.set_trace()\n",
    "\n",
    "        texture_patch = gen_local_patch(patchsize, batch_size, eroded_seg, seg, outputlll)\n",
    "        gt_texture_patch = gen_local_patch(patchsize, batch_size, eroded_seg, seg, targetlll)\n",
    "\n",
    "\n",
    "    if args.color_space == 'lab':\n",
    "        out_img = vis_image(denormalize_lab(outputG.data.double().cpu()),\n",
    "                            args.color_space)\n",
    "        temp_labout = denormalize_lab(texture_patch.data.double().cpu())\n",
    "        temp_labout[:,1:3,:,:] = 0\n",
    "        \n",
    "        temp_labgt = denormalize_lab(gt_texture_patch.data.double().cpu())\n",
    "        temp_labgt[:,1:3,:,:] = 0\n",
    "        temp_out =vis_image(temp_labout,args.color_space) #torch.cat((patches[0].data.double().cpu(),patches[0].data.double().cpu(),patches[0].data.double().cpu()),1)\n",
    "        #temp_out = (temp_out + 1 )/2\n",
    "                            \n",
    "        temp_gt =vis_image(temp_labgt,\n",
    "                            args.color_space) #torch.cat((patches[1].data.double().cpu(),patches[1].data.double().cpu(),patches[1].data.double().cpu()),1)\n",
    "\n",
    "        if args.input_texture_patch == 'original_image':\n",
    "            inp_img = vis_patch(denormalize_lab(img.cpu()),\n",
    "                                denormalize_lab(skg.cpu()),\n",
    "                                texture_loc,\n",
    "                                args.color_space)\n",
    "        elif args.input_texture_patch == 'dtd_texture':\n",
    "            inp_img = vis_patch(denormalize_lab(txt.cpu()),\n",
    "                                denormalize_lab(skg.cpu()),\n",
    "                                texture_loc,\n",
    "                                args.color_space)\n",
    "        tar_img = vis_image(denormalize_lab(img.cpu()),\n",
    "                            args.color_space)\n",
    "        skg_img = vis_image(denormalize_lab(skg.cpu()),\n",
    "                            args.color_space)\n",
    "        txt_img = vis_image(denormalize_lab(txt.cpu()),\n",
    "                            args.color_space)\n",
    "    elif args.color_space == 'rgb':\n",
    "\n",
    "        out_img = vis_image(denormalize_rgb(outputG.data.double().cpu()),\n",
    "                            args.color_space)\n",
    "        inp_img = vis_patch(denormalize_rgb(img.cpu()),\n",
    "                            denormalize_rgb(skg.cpu()),\n",
    "                            texture_loc,\n",
    "                            args.color_space)\n",
    "        tar_img = vis_image(denormalize_rgb(img.cpu()),\n",
    "                            args.color_space)\n",
    "\n",
    "    out_final = [x*0 for x in txt_img] \n",
    "    gt_final = [x*0 for x in txt_img] \n",
    "    out_img = [x * 255 for x in out_img]  # (out_img*255)#.astype('uint8')\n",
    "    skg_img = [x * 255 for x in skg_img]  # (out_img*255)#.astype('uint8')\n",
    "    out_patch = [x * 255 for x in temp_out]\n",
    "    gt_patch = [x * 255 for x in temp_gt]    # out_img=np.transpose(out_img,(2,0,1))\n",
    "    for t_i in range(bs):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        patchsize = int(args.local_texture_size)\n",
    "        out_final[t_i][:,0:patchsize,0:patchsize] = out_patch[t_i][:,:,:]# .append(np.resize(out_patch[t_i], (3,w,h)))\n",
    "        gt_final[t_i][:,0:patchsize,0:patchsize] =gt_patch[t_i][:,:,:]#gt_final.append(np.resize(gt_patch[t_i], (3,w,h)))\n",
    "   \n",
    "    \n",
    "    # out_img=np.transpose(out_img,(2,0,1))\n",
    "\n",
    "    txt_img = [x * 255 for x in txt_img]    \n",
    "    inp_img = [x * 255 for x in inp_img]  # (inp_img*255)#.astype('uint8')\n",
    "    # inp_img=np.transpose(inp_img,(2,0,1))\n",
    "\n",
    "    tar_img = [x * 255 for x in tar_img]  # (tar_img*255)#.astype('uint8')\n",
    "    # tar_img=np.transpose(tar_img,(2,0,1))\n",
    "    #import pdb; pdb.set_trace()\n",
    "    \n",
    "    #segment_img = vis_image((eroded_seg.cpu()), args.color_space)\n",
    "    #import pdb; pdb.set_trace()\n",
    "    segment_img = [x * 255 for x in eroded_seg.cpu().numpy()]  # segment_img=(segment_img*255)#.astype('uint8')\n",
    "    # segment_img=np.transpose(segment_img,(2,0,1))\n",
    "    #import pdb; pdb.set_trace()\n",
    "    for i_ in range(len(out_img)):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        imgs.append(skg_img[i_])\n",
    "        imgs.append(txt_img[i_])\n",
    "        imgs.append(inp_img[i_])\n",
    "        imgs.append(out_img[i_])\n",
    "        imgs.append(segment_img[i_])\n",
    "        imgs.append(tar_img[i_])\n",
    "        imgs.append(out_final[i_])\n",
    "        imgs.append(gt_final[i_])\n",
    "\n",
    "    # for idx, img in enumerate(imgs):\n",
    "    #     print(idx, type(img), img.shape)\n",
    "\n",
    "    vis.images(imgs, win='output', opts=dict(title='Output images'))\n",
    "    # vis.image(inp_img,win='input',opts=dict(title='input'))\n",
    "    # vis.image(tar_img,win='target',opts=dict(title='target'))\n",
    "    # vis.image(segment_img,win='segment',opts=dict(title='segment'))\n",
    "    vis.line(np.array(loss_graph[\"gs\"]), win='gs', opts=dict(title='G-Style Loss'))\n",
    "    vis.line(np.array(loss_graph[\"g\"]), win='g', opts=dict(title='G Total Loss'))\n",
    "    vis.line(np.array(loss_graph[\"gd\"]), win='gd', opts=dict(title='G-Discriminator Loss'))\n",
    "    vis.line(np.array(loss_graph[\"gf\"]), win='gf', opts=dict(title='G-Feature Loss'))\n",
    "    vis.line(np.array(loss_graph[\"gpl\"]), win='gpl', opts=dict(title='G-Pixel Loss-L'))\n",
    "    vis.line(np.array(loss_graph[\"gpab\"]), win='gpab', opts=dict(title='G-Pixel Loss-AB'))\n",
    "    vis.line(np.array(loss_graph[\"d\"]), win='d', opts=dict(title='D Loss'))\n",
    "    if args.local_texture_size != -1:\n",
    "        vis.line(np.array(loss_graph[\"dl\"]), win='dl', opts=dict(title='D Local Loss'))\n",
    "        vis.line(np.array(loss_graph[\"gdl\"]), win='gdl', opts=dict(title='G D Local Loss'))\n",
    "    \n",
    "def train(model, train_loader, val_loader, input_stack, target_img, target_texture,\n",
    "          segment, label,label_local, extract_content, extract_style, loss_graph, vis, epoch, args):\n",
    "\n",
    "    netG = model[\"netG\"]\n",
    "    netD = model[\"netD\"]\n",
    "    netD_local = model[\"netD_local\"]\n",
    "    criterion_gan = model[\"criterion_gan\"]\n",
    "    criterion_pixel_l = model[\"criterion_pixel_l\"]\n",
    "    criterion_pixel_ab = model[\"criterion_pixel_ab\"]\n",
    "    criterion_feat = model[\"criterion_feat\"]\n",
    "    criterion_style = model[\"criterion_style\"]\n",
    "    criterion_texturegan = model[\"criterion_texturegan\"]\n",
    "    real_label = model[\"real_label\"]\n",
    "    fake_label = model[\"fake_label\"]\n",
    "    optimizerD = model[\"optimizerD\"]\n",
    "    optimizerD_local = model[\"optimizerD_local\"]\n",
    "    optimizerG = model[\"optimizerG\"]\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        print(\"Epoch: {0}       Iteration: {1}\".format(epoch, i))\n",
    "        # Detach is apparently just creating new Variable with cut off reference to previous node, so shouldn't effect the original\n",
    "        # But just in case, let's do G first so that detaching G during D update don't do anything weird\n",
    "        ############################\n",
    "        # (1) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "\n",
    "        img, skg, seg, eroded_seg, txt = data  # LAB with negeative value\n",
    "        if random.random() < 0.5:\n",
    "            txt = img\n",
    "        # output img/skg/seg rgb between 0-1\n",
    "        # output img/skg/seg lab between 0-100, -128-128\n",
    "        if args.color_space == 'lab':\n",
    "            img = normalize_lab(img)\n",
    "            skg = normalize_lab(skg)\n",
    "            txt = normalize_lab(txt)\n",
    "            seg = normalize_seg(seg)\n",
    "            eroded_seg = normalize_seg(eroded_seg)\n",
    "            # seg = transforms.normalize_lab(seg)\n",
    "        elif args.color_space == 'rgb':\n",
    "            img = normalize_rgb(img)\n",
    "            skg = normalize_rgb(skg)\n",
    "            txt = normalize_rgb(txt)\n",
    "            # seg=transforms.normalize_rgb(seg)\n",
    "        # print seg\n",
    "        if not args.use_segmentation_patch:\n",
    "            seg.fill_(1)\n",
    "         \n",
    "        bs, w, h = seg.size()\n",
    "\n",
    "        seg = seg.view(bs, 1, w, h)\n",
    "        seg = torch.cat((seg, seg, seg), 1)\n",
    "        eroded_seg = eroded_seg.view(bs, 1, w, h)\n",
    "        \n",
    "        # import pdb; pdb.set_trace()\n",
    "\n",
    "        temp = torch.ones(seg.size()) * (1 - seg).float()\n",
    "        temp[:, 1, :, :] = 0  # torch.ones(seg[:,1,:,:].size())*(1-seg[:,1,:,:]).float()\n",
    "        temp[:, 2, :, :] = 0  # torch.ones(seg[:,2,:,:].size())*(1-seg[:,2,:,:]).float()\n",
    "\n",
    "        txt = txt.float() * seg.float() + temp\n",
    "        #tic = time.time()\n",
    "        if args.input_texture_patch == 'original_image':\n",
    "            inp, _ = gen_input_rand(img, skg, eroded_seg[:, 0, :, :], args.patch_size_min, args.patch_size_max,\n",
    "                                    args.num_input_texture_patch)\n",
    "        elif args.input_texture_patch == 'dtd_texture':\n",
    "            inp, _ = gen_input_rand(txt, skg, eroded_seg[:, 0, :, :], args.patch_size_min, args.patch_size_max,\n",
    "                                    args.num_input_texture_patch)\n",
    "        #print(time.time()-tic)\n",
    "        batch_size, _, _, _ = img.size()\n",
    "\n",
    "        img = img.cuda()\n",
    "        skg = skg.cuda()\n",
    "        seg = seg.cuda()\n",
    "        eroded_seg = eroded_seg.cuda()\n",
    "        txt = txt.cuda()\n",
    "\n",
    "        inp = inp.cuda()\n",
    "\n",
    "        input_stack.resize_as_(inp.float()).copy_(inp)\n",
    "        target_img.resize_as_(img.float()).copy_(img)\n",
    "        segment.resize_as_(seg.float()).copy_(seg)\n",
    "        target_texture.resize_as_(txt.float()).copy_(txt)\n",
    "        \n",
    "        inv_idx = torch.arange(target_texture.size(0)-1, -1, -1).long().cuda()\n",
    "        target_texture_inv = target_texture.index_select(0, inv_idx)\n",
    "\n",
    "        assert torch.max(seg) <= 1\n",
    "        assert torch.max(eroded_seg) <= 1\n",
    "\n",
    "        inputv = Variable(input_stack)\n",
    "        gtimgv = Variable(target_img)\n",
    "        segv = Variable(segment)\n",
    "        txtv = Variable(target_texture)\n",
    "        txtv_inv = Variable(target_texture_inv)\n",
    "        \n",
    "        outputG = netG(inputv)\n",
    "\n",
    "        outputl, outputa, outputb = torch.chunk(outputG, 3, dim=1)\n",
    "\n",
    "        gtl, gta, gtb = torch.chunk(gtimgv, 3, dim=1)\n",
    "        txtl, txta, txtb = torch.chunk(txtv, 3, dim=1)\n",
    "        txtl_inv,txta_inv,txtb_inv = torch.chunk(txtv_inv,3,dim=1)\n",
    "\n",
    "        outputab = torch.cat((outputa, outputb), 1)\n",
    "        gtab = torch.cat((gta, gtb), 1)\n",
    "        txtab = torch.cat((txta, txtb), 1)\n",
    "\n",
    "        if args.color_space == 'lab':\n",
    "            outputlll = (torch.cat((outputl, outputl, outputl), 1))\n",
    "            gtlll = (torch.cat((gtl, gtl, gtl), 1))\n",
    "            txtlll = torch.cat((txtl, txtl, txtl), 1)\n",
    "        elif args.color_space == 'rgb':\n",
    "            outputlll = outputG  # (torch.cat((outputl,outputl,outputl),1))\n",
    "            gtlll = gtimgv  # (torch.cat((targetl,targetl,targetl),1))\n",
    "            txtlll = txtv\n",
    "        if args.loss_texture == 'original_image':\n",
    "            targetl = gtl\n",
    "            targetab = gtab\n",
    "            targetlll = gtlll\n",
    "        else:\n",
    "            # if args.loss_texture == 'texture_mask':\n",
    "            # remove baskground dtd\n",
    "            #     txtl = segv[:,0:1,:,:]*txtl\n",
    "            #     txtab=segv[:,1:3,:,:]*txtab\n",
    "            #     txtlll=segv*txtlll\n",
    "            # elif args.loss_texture == 'texture_patch':\n",
    "\n",
    "            targetl = txtl\n",
    "            targetab = txtab\n",
    "            targetlll = txtlll\n",
    "\n",
    "        ################## Global Pixel ab Loss ############################\n",
    "        \n",
    "        err_pixel_ab = args.pixel_weight_ab * criterion_pixel_ab(outputab, targetab)\n",
    "\n",
    "        ################## Global Feature Loss############################\n",
    "        \n",
    "        out_feat = extract_content(renormalize(outputlll))[0]\n",
    "\n",
    "        gt_feat = extract_content(renormalize(gtlll))[0]\n",
    "        err_feat = args.feature_weight * criterion_feat(out_feat, gt_feat.detach())\n",
    "\n",
    "        ################## Global D Adversarial Loss ############################\n",
    "        \n",
    "        netD.zero_grad()\n",
    "        label_ = Variable(label)\n",
    "        \n",
    "        #return outputl, txtl\n",
    "        if args.color_space == 'lab':\n",
    "            outputD = netD(outputl)\n",
    "        elif args.color_space == 'rgb':\n",
    "            outputD = netD(outputG)\n",
    "        # D_G_z2 = outputD.data.mean()\n",
    "\n",
    "        label.resize_(outputD.data.size())\n",
    "        labelv = Variable(label.fill_(real_label))\n",
    "\n",
    "        err_gan = args.discriminator_weight * criterion_gan(outputD, labelv)\n",
    "        err_pixel_l = 0\n",
    "        ################## Global Pixel L Loss ############################\n",
    "             \n",
    "        err_pixel_l = args.global_pixel_weight_l * criterion_pixel_l(outputl, targetl)\n",
    "        if args.local_texture_size == -1:  # global, no loss patch\n",
    "            \n",
    "            ################## Global Style Loss ############################\n",
    "            \n",
    "            output_style_feat = extract_style(outputlll)\n",
    "            target_style_feat = extract_style(targetlll)\n",
    "            \n",
    "            gram = GramMatrix()\n",
    "\n",
    "            err_style = 0\n",
    "            for m in range(len(output_style_feat)):\n",
    "                gram_y = gram(output_style_feat[m])\n",
    "                gram_s = gram(target_style_feat[m])\n",
    "\n",
    "                err_style += args.style_weight * criterion_style(gram_y, gram_s.detach())\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            err_texturegan = 0\n",
    "                        \n",
    "        else: # local loss patch\n",
    "            err_style = 0\n",
    "            \n",
    "            patchsize = args.local_texture_size\n",
    "            \n",
    "            netD_local.zero_grad()\n",
    "             \n",
    "            for p in range(args.num_local_texture_patch):\n",
    "                texture_patch = gen_local_patch(patchsize, batch_size, eroded_seg,seg, outputlll)\n",
    "                gt_texture_patch = gen_local_patch(patchsize, batch_size, eroded_seg,seg, targetlll)\n",
    "\n",
    "                texture_patchl = gen_local_patch(patchsize, batch_size, eroded_seg, seg,outputl)\n",
    "                gt_texture_patchl = gen_local_patch(patchsize, batch_size, eroded_seg,seg, targetl)\n",
    "\n",
    "                ################## Local Style Loss ############################\n",
    "\n",
    "                output_style_feat = extract_style(texture_patch)\n",
    "                target_style_feat = extract_style(gt_texture_patch)\n",
    "\n",
    "                gram = GramMatrix()\n",
    "\n",
    "\n",
    "                for m in range(len(output_style_feat)):\n",
    "                    gram_y = gram(output_style_feat[m])\n",
    "                    gram_s = gram(target_style_feat[m])\n",
    "\n",
    "                    err_style += args.style_weight * criterion_style(gram_y, gram_s.detach())\n",
    "\n",
    "                ################## Local Pixel L Loss ############################\n",
    "\n",
    "                err_pixel_l += args.local_pixel_weight_l * criterion_pixel_l(texture_patchl, gt_texture_patchl)\n",
    "            \n",
    "            \n",
    "                ################## Local D Loss ############################\n",
    "                \n",
    "                label_ = Variable(label)\n",
    "                err_texturegan = 0\n",
    "            \n",
    "                outputD_local = netD_local(torch.cat((texture_patchl, gt_texture_patchl),1))\n",
    "\n",
    "                label_local.resize_(outputD_local.data.size())\n",
    "                labelv_local = Variable(label_local.fill_(real_label))\n",
    "\n",
    "                err_texturegan += args.discriminator_local_weight * criterion_texturegan(outputD_local, labelv_local)\n",
    "            loss_graph[\"gdl\"].append(err_texturegan.data[0])\n",
    "        \n",
    "        ####################################\n",
    "        err_G = err_pixel_l + err_pixel_ab + err_gan + err_feat + err_style + err_texturegan\n",
    "        \n",
    "        err_G.backward(retain_variables=True)\n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "        loss_graph[\"g\"].append(err_G.data[0])\n",
    "        loss_graph[\"gpl\"].append(err_pixel_l.data[0])\n",
    "        loss_graph[\"gpab\"].append(err_pixel_ab.data[0])\n",
    "        loss_graph[\"gd\"].append(err_gan.data[0])\n",
    "        loss_graph[\"gf\"].append(err_feat.data[0])\n",
    "        loss_graph[\"gs\"].append(err_style.data[0])\n",
    "            \n",
    "\n",
    "        print('G:', err_G.data[0])\n",
    "\n",
    "        ############################\n",
    "        # (2) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        \n",
    "        \n",
    "        netD.zero_grad()\n",
    "\n",
    "        labelv = Variable(label)\n",
    "        if args.color_space == 'lab':\n",
    "            outputD = netD(gtl)\n",
    "        elif args.color_space == 'rgb':\n",
    "            outputD = netD(gtimgv)\n",
    "\n",
    "        label.resize_(outputD.data.size())\n",
    "        labelv = Variable(label.fill_(real_label))\n",
    "\n",
    "        errD_real = criterion_gan(outputD, labelv)\n",
    "        errD_real.backward()\n",
    "\n",
    "        score = Variable(torch.ones(batch_size))\n",
    "        _, cd, wd, hd = outputD.size()\n",
    "        D_output_size = cd * wd * hd\n",
    "\n",
    "        clamped_output_D = outputD.clamp(0, 1)\n",
    "        clamped_output_D = torch.round(clamped_output_D)\n",
    "        for acc_i in range(batch_size):\n",
    "            score[acc_i] = torch.sum(clamped_output_D[acc_i]) / D_output_size\n",
    "\n",
    "        real_acc = torch.mean(score)\n",
    "\n",
    "        if args.color_space == 'lab':\n",
    "            outputD = netD(outputl.detach())\n",
    "        elif args.color_space == 'rgb':\n",
    "            outputD = netD(outputG.detach())\n",
    "        label.resize_(outputD.data.size())\n",
    "        labelv = Variable(label.fill_(fake_label))\n",
    "\n",
    "        errD_fake = criterion_gan(outputD, labelv)\n",
    "        errD_fake.backward()\n",
    "        score = Variable(torch.ones(batch_size))\n",
    "        _, cd, wd, hd = outputD.size()\n",
    "        D_output_size = cd * wd * hd\n",
    "\n",
    "        clamped_output_D = outputD.clamp(0, 1)\n",
    "        clamped_output_D = torch.round(clamped_output_D)\n",
    "        for acc_i in range(batch_size):\n",
    "            score[acc_i] = torch.sum(clamped_output_D[acc_i]) / D_output_size\n",
    "\n",
    "        fake_acc = torch.mean(1 - score)\n",
    "\n",
    "        D_acc = (real_acc + fake_acc) / 2\n",
    "\n",
    "        if D_acc.data[0] < args.threshold_D_max:\n",
    "            # D_G_z1 = output.data.mean()\n",
    "            errD = errD_real + errD_fake\n",
    "            loss_graph[\"d\"].append(errD.data[0])\n",
    "            optimizerD.step()\n",
    "        else:\n",
    "            loss_graph[\"d\"].append(0)\n",
    "\n",
    "        print('D:', 'real_acc', \"%.2f\" % real_acc.data[0], 'fake_acc', \"%.2f\" % fake_acc.data[0], 'D_acc', D_acc.data[0])\n",
    "\n",
    "        ############################\n",
    "        # (2) Update D local network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        \n",
    "        if args.local_texture_size != -1:\n",
    "            patchsize = args.local_texture_size\n",
    "            x1 = int(rand_between(patchsize, args.image_size - patchsize))\n",
    "            y1 = int(rand_between(patchsize, args.image_size - patchsize))\n",
    "\n",
    "            x2 = int(rand_between(patchsize, args.image_size - patchsize))\n",
    "            y2 = int(rand_between(patchsize, args.image_size - patchsize))\n",
    "\n",
    "            netD_local.zero_grad()\n",
    "\n",
    "            labelv = Variable(label)\n",
    "            if args.color_space == 'lab':\n",
    "                outputD_local = netD_local(torch.cat((targetl[:, :, x1:(x1 + patchsize), y1:(y1 + patchsize)],targetl[:, :, x2:(x2 + patchsize), y2:(y2 + patchsize)]),1))#netD_local(targetl)\n",
    "            elif args.color_space == 'rgb':\n",
    "                outputD = netD(gtimgv)\n",
    "\n",
    "            label.resize_(outputD_local.data.size())\n",
    "            labelv = Variable(label.fill_(real_label))\n",
    "\n",
    "            errD_real_local = criterion_texturegan(outputD_local, labelv)\n",
    "            errD_real_local.backward(retain_variables=True)\n",
    "\n",
    "            score = Variable(torch.ones(batch_size))\n",
    "            _, cd, wd, hd = outputD_local.size()\n",
    "            D_output_size = cd * wd * hd\n",
    "\n",
    "            clamped_output_D = outputD_local.clamp(0, 1)\n",
    "            clamped_output_D = torch.round(clamped_output_D)\n",
    "            for acc_i in range(batch_size):\n",
    "                score[acc_i] = torch.sum(clamped_output_D[acc_i]) / D_output_size\n",
    "\n",
    "            realreal_acc = torch.mean(score)\n",
    "\n",
    "            \n",
    "\n",
    "            x1 = int(rand_between(patchsize, args.image_size - patchsize))\n",
    "            y1 = int(rand_between(patchsize, args.image_size - patchsize))\n",
    "\n",
    "            x2 = int(rand_between(patchsize, args.image_size - patchsize))\n",
    "            y2 = int(rand_between(patchsize, args.image_size - patchsize))\n",
    "\n",
    "\n",
    "            if args.color_space == 'lab':\n",
    "                #outputD_local = netD_local(torch.cat((txtl[:, :, x1:(x1 + patchsize), y1:(y1 + patchsize)],outputl[:, :, x2:(x2 + patchsize), y2:(y2 + patchsize)]),1))#outputD = netD(outputl.detach())\n",
    "                outputD_local = netD_local(torch.cat((texture_patchl, gt_texture_patchl),1))\n",
    "            elif args.color_space == 'rgb':\n",
    "                outputD = netD(outputG.detach())\n",
    "            label.resize_(outputD_local.data.size())\n",
    "            labelv = Variable(label.fill_(fake_label))\n",
    "\n",
    "            errD_fake_local = criterion_gan(outputD_local, labelv)\n",
    "            errD_fake_local.backward()\n",
    "            score = Variable(torch.ones(batch_size))\n",
    "            _, cd, wd, hd = outputD_local.size()\n",
    "            D_output_size = cd * wd * hd\n",
    "\n",
    "            clamped_output_D = outputD_local.clamp(0, 1)\n",
    "            clamped_output_D = torch.round(clamped_output_D)\n",
    "            for acc_i in range(batch_size):\n",
    "                score[acc_i] = torch.sum(clamped_output_D[acc_i]) / D_output_size\n",
    "\n",
    "            fakefake_acc = torch.mean(1 - score)\n",
    "\n",
    "            D_acc = (realreal_acc +fakefake_acc) / 2\n",
    "\n",
    "            if D_acc.data[0] < args.threshold_D_max:\n",
    "                # D_G_z1 = output.data.mean()\n",
    "                errD_local = errD_real_local + errD_fake_local\n",
    "                loss_graph[\"dl\"].append(errD_local.data[0])\n",
    "                optimizerD_local.step()\n",
    "            else:\n",
    "                loss_graph[\"dl\"].append(0)\n",
    "\n",
    "            print('D local:', 'real real_acc', \"%.2f\" % realreal_acc.data[0], 'fake fake_acc', \"%.2f\" % fakefake_acc.data[0], 'D_acc', D_acc.data[0])\n",
    "            #if i % args.save_every == 0:\n",
    "             #   save_network(netD_local, 'D_local', epoch, i, args)\n",
    "\n",
    "        if i % args.save_every == 0:\n",
    "            save_network(netG, 'G', epoch, i, args)\n",
    "            save_network(netD, 'D', epoch, i, args)\n",
    "            save_network(netD_local, 'D_local', epoch, i, args)\n",
    "            \n",
    "        if i % args.visualize_every == 0:\n",
    "            visualize_training(netG, val_loader, input_stack, target_img,target_texture, segment, vis, loss_graph, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys, os\n",
    "import visdom\n",
    "import torchvision.models as models\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "# from dataloader.imfol import ImageFolder\n",
    "\n",
    "# from utils import transforms as transforms\n",
    "# from models import scribbler, discriminator, texturegan, define_G, weights_init, \\\n",
    "#     scribbler_dilate_128, FeatureExtractor, load_network\n",
    "# from train import train\n",
    "\n",
    "# import argparser\n",
    "\n",
    "def get_transforms(args):\n",
    "    transforms_list = [\n",
    "        RandomSizedCrop(args.image_size, args.resize_min, args.resize_max),\n",
    "        RandomHorizontalFlip(),\n",
    "        toTensor()\n",
    "    ]\n",
    "    if args.color_space == 'lab':\n",
    "        transforms_list.insert(2, toLAB())\n",
    "    elif args.color_space == 'rgb':\n",
    "        transforms_list.insert(2, toRGB('RGB'))\n",
    "\n",
    "    transforms = Compose(transforms_list)\n",
    "    return transforms\n",
    "\n",
    "\n",
    "def get_models(args):\n",
    "    sigmoid_flag = 1\n",
    "    if args.gan == 'lsgan':\n",
    "        sigmoid_flag = 0\n",
    "\n",
    "    if args.model == 'scribbler':\n",
    "        netG = scribbler.Scribbler(5, 3, 32)\n",
    "    elif args.model == 'texturegan':\n",
    "        netG = texturegan.TextureGAN(5, 3, 32)\n",
    "    elif args.model == 'pix2pix':\n",
    "        netG = define_G(5, 3, 32)\n",
    "    elif args.model == 'scribbler_dilate_128':\n",
    "        netG = scribbler_dilate_128.ScribblerDilate128(5, 3, 32)\n",
    "    else:\n",
    "        print(args.model + ' not support. Using Scribbler model')\n",
    "        netG = scribbler.Scribbler(5, 3, 32)\n",
    "\n",
    "    if args.color_space == 'lab':\n",
    "        netD = discriminator.Discriminator(1, 32, sigmoid_flag)\n",
    "        netD_local = discriminator.LocalDiscriminator(2, 32, sigmoid_flag)\n",
    "    elif args.color_space == 'rgb':\n",
    "        netD = discriminator.Discriminator(3, 32, sigmoid_flag)\n",
    "\n",
    "    if args.load == -1:\n",
    "        netG.apply(weights_init)\n",
    "    else:\n",
    "        load_network(netG, 'G', args.load_epoch, args.load, args)\n",
    "\n",
    "    if args.load_D == -1:\n",
    "        netD.apply(weights_init)\n",
    "    else:\n",
    "        load_network(netD, 'D', args.load_epoch, args.load_D, args)\n",
    "        load_network(netD_local, 'D_local', args.load_epoch, args.load_D, args)\n",
    "    return netG, netD, netD_local\n",
    "\n",
    "\n",
    "def get_criterions(args):\n",
    "    if args.gan == 'lsgan':\n",
    "        criterion_gan = nn.MSELoss()\n",
    "    elif args.gan == 'dcgan':\n",
    "        criterion_gan = nn.BCELoss()\n",
    "    else:\n",
    "        print(\"Undefined GAN type. Defaulting to LSGAN\")\n",
    "        criterion_gan = nn.MSELoss()\n",
    "\n",
    "    # criterion_l1 = nn.L1Loss()\n",
    "    criterion_pixel_l = nn.MSELoss()\n",
    "    criterion_pixel_ab = nn.MSELoss()\n",
    "    criterion_style = nn.MSELoss()\n",
    "    criterion_feat = nn.MSELoss()\n",
    "    criterion_texturegan = nn.MSELoss()\n",
    "\n",
    "    return criterion_gan, criterion_pixel_l, criterion_pixel_ab, criterion_style, criterion_feat, criterion_texturegan\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    #with torch.cuda.device(args.gpu):\n",
    "    layers_map = {'relu4_2': '22', 'relu2_2': '8', 'relu3_2': '13','relu1_2': '4'}\n",
    "\n",
    "    vis = visdom.Visdom(port=args.display_port)\n",
    "\n",
    "    loss_graph = {\n",
    "        \"g\": [],\n",
    "        \"gd\": [],\n",
    "        \"gf\": [],\n",
    "        \"gpl\": [],\n",
    "        \"gpab\": [],\n",
    "        \"gs\": [],\n",
    "        \"d\": [],\n",
    "        \"gdl\": [],\n",
    "        \"dl\": [],\n",
    "    }\n",
    "\n",
    "    # for rgb the change is to feed 3 channels to D instead of just 1. and feed 3 channels to vgg.\n",
    "    # can leave pixel separate between r and gb for now. assume user use the same weights\n",
    "    transforms = get_transforms(args)\n",
    "\n",
    "    if args.color_space == 'rgb':\n",
    "        args.pixel_weight_ab = args.pixel_weight_rgb\n",
    "        args.pixel_weight_l = args.pixel_weight_rgb\n",
    "\n",
    "    rgbify = toRGB()\n",
    "\n",
    "    train_dataset = ImageFolder('train_img/wendy', args.data_path, transforms)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    val_dataset = ImageFolder('val', args.data_path, transforms)\n",
    "    indices = torch.randperm(len(val_dataset))\n",
    "    val_display_size = args.batch_size\n",
    "    val_display_sampler = SequentialSampler(indices[:val_display_size])\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=val_display_size, sampler=val_display_sampler)\n",
    "    # renormalize = transforms.Normalize(mean=[+0.5+0.485, +0.5+0.456, +0.5+0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    feat_model = models.vgg19(pretrained=True)\n",
    "    netG, netD, netD_local = get_models(args)\n",
    "\n",
    "    criterion_gan, criterion_pixel_l, criterion_pixel_ab, criterion_style, criterion_feat,criterion_texturegan = get_criterions(args)\n",
    "\n",
    "\n",
    "    real_label = 1\n",
    "    fake_label = 0\n",
    "\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=args.learning_rate_D, betas=(0.5, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=args.learning_rate, betas=(0.5, 0.999))\n",
    "    optimizerD_local = optim.Adam(netD_local.parameters(), lr=args.learning_rate_D_local, betas=(0.5, 0.999))\n",
    "\n",
    "    with torch.cuda.device(args.gpu):\n",
    "        netG.cuda()\n",
    "        netD.cuda()\n",
    "        netD_local.cuda()\n",
    "        feat_model.cuda()\n",
    "        criterion_gan.cuda()\n",
    "        criterion_pixel_l.cuda()\n",
    "        criterion_pixel_ab.cuda()\n",
    "        criterion_feat.cuda()\n",
    "        criterion_texturegan.cuda()\n",
    "\n",
    "        input_stack = torch.FloatTensor().cuda()\n",
    "        target_img = torch.FloatTensor().cuda()\n",
    "        target_texture = torch.FloatTensor().cuda()\n",
    "        segment = torch.FloatTensor().cuda()\n",
    "        label = torch.FloatTensor(args.batch_size).cuda()\n",
    "        label_local = torch.FloatTensor(args.batch_size).cuda()\n",
    "        extract_content = FeatureExtractor(feat_model.features, [layers_map[args.content_layers]])\n",
    "        extract_style = FeatureExtractor(feat_model.features,\n",
    "                                         [layers_map[x.strip()] for x in args.style_layers.split(',')])\n",
    "\n",
    "        model = {\n",
    "            \"netG\": netG,\n",
    "            \"netD\": netD,\n",
    "            \"netD_local\": netD_local,\n",
    "            \"criterion_gan\": criterion_gan,\n",
    "            \"criterion_pixel_l\": criterion_pixel_l,\n",
    "            \"criterion_pixel_ab\": criterion_pixel_ab,\n",
    "            \"criterion_feat\": criterion_feat,\n",
    "            \"criterion_style\": criterion_style,\n",
    "            \"criterion_texturegan\": criterion_texturegan,\n",
    "            \"real_label\": real_label,\n",
    "            \"fake_label\": fake_label,\n",
    "            \"optimizerD\": optimizerD,\n",
    "            \"optimizerD_local\": optimizerD_local,\n",
    "            \"optimizerG\": optimizerG\n",
    "        }\n",
    "\n",
    "        for epoch in range(args.load_epoch, args.num_epoch):\n",
    "            train(model, train_loader, val_loader, input_stack, target_img, target_texture,\n",
    "                  segment, label, label_local,extract_content, extract_style, loss_graph, vis, epoch, args)\n",
    "            #break\n",
    "# if __name__ == '__main__':\n",
    "#     args = parse_arguments()\n",
    "#     main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting easydict\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/c5/5757886c4f538c1b3f95f6745499a24bffa389a805dee92d093e2d9ba7db/easydict-1.9.tar.gz\n",
      "Building wheels for collected packages: easydict\n",
      "  Building wheel for easydict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/spuliz/Library/Caches/pip/wheels/9a/88/ec/085d92753646b0eda1b7df49c7afe51a6ecc496556d3012e2e\n",
      "Successfully built easydict\n",
      "Installing collected packages: easydict\n",
      "Successfully installed easydict-1.9\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m visdom.server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
